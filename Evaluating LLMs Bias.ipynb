{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ecae91e",
   "metadata": {},
   "source": [
    "#  Evaluating Biases in Large Language Models (LLMs) using WEAT and Demographic Diversity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b315a46",
   "metadata": {},
   "source": [
    "#### Word Embedding Association Test (WEAT)\n",
    "What are Word Embeddings?\n",
    "\n",
    "- A brief overview of word embeddings (e.g., Word2Vec, GloVe) and their significance in NLP.\n",
    "Mathematical representation of word embeddings.\n",
    "\n",
    "Introduction to WEAT\n",
    "\n",
    "Objective: \n",
    "- Measure the strength and direction of associations between word embeddings and predefined categories.\n",
    "- Real-world implications of biases in word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10fa2c36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T19:54:02.712130Z",
     "start_time": "2023-11-04T19:54:02.705788Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b1479",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "This dictionary contains 3-dimensional embeddings (vectors) for various words.\n",
    "\n",
    "In a real-world scenario, these embeddings would be derived from models like Word2Vec, GloVe, or large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fac571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T19:59:04.188269Z",
     "start_time": "2023-11-04T19:59:04.180651Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embeddings = {\n",
    "    'doctor': np.array([0.1, 0.3, 0.5]),\n",
    "    'engineer': np.array([0.2, 0.4, 0.2]),\n",
    "    'scientist': np.array([0.3, 0.1, 0.4]),\n",
    "    'nurse': np.array([0.5, 0.1, 0.3]),\n",
    "    'teacher': np.array([0.4, 0.2, 0.1]),\n",
    "    'receptionist': np.array([0.3, 0.4, 0.3]),\n",
    "    'man': np.array([0.5, 0.5, 0.5]),\n",
    "    'male': np.array([0.5, 0.4, 0.5]),\n",
    "    'boy': np.array([0.5, 0.5, 0.4]),\n",
    "    'woman': np.array([0.5, 0.2, 0.3]),\n",
    "    'female': np.array([0.5, 0.3, 0.3]),\n",
    "    'girl': np.array([0.5, 0.3, 0.4])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c075ad6",
   "metadata": {},
   "source": [
    "### Defining Word Sets\n",
    "\n",
    "- X and Y are target word sets. In our example, they represent different occupations.\n",
    "- A and B are attribute word sets, representing gender terms in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f082afd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T20:00:31.979938Z",
     "start_time": "2023-11-04T20:00:31.973291Z"
    }
   },
   "outputs": [],
   "source": [
    "#let's define the sets\n",
    "X = ['doctor', 'engineer', 'scientist']\n",
    "Y = ['nurse', 'teacher', 'receptionist']\n",
    "A = ['man', 'male', 'boy']\n",
    "B = ['woman', 'female', 'girl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e60c2fb",
   "metadata": {},
   "source": [
    "### Computing Differential Association\n",
    "\n",
    "- The function s computes the differential association of a word w with the sets X and Y.\n",
    "- For each word in X, we compute its cosine similarity with w and then take the mean of these values to get sim_X.\n",
    "- Similarly, we compute the average cosine similarity between w and each word in Y to get sim_Y.\n",
    "- The function returns the difference between sim_X and sim_Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34855ec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T20:03:01.187223Z",
     "start_time": "2023-11-04T20:03:01.180137Z"
    }
   },
   "outputs": [],
   "source": [
    "def s(w, X, Y):\n",
    "    return sum([cosine_similarity(word_embeddings[w].reshape(1, -1), word_embeddings[x].reshape(1, -1)) for x in X]) - ([cosine_similarity(word_embeddings[w].reshape(1, -1), word_embeddings[y].reshape(1, -1)) for y in Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25aca2",
   "metadata": {},
   "source": [
    "### Calculating the WEAT Score\n",
    "- For each word in set A, we compute its differential association with X and Y and sum these values.\n",
    "- Similarly, we compute the sum of differential associations for each word in set B.\n",
    "- The WEAT score is the difference between the two sums.\n",
    "- A positive WEAT score indicates that, on average, words in A are more strongly associated with words in X than words in B are. Conversely, a negative score indicates a stronger association between B and X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08484f04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T20:04:03.153376Z",
     "start_time": "2023-11-04T20:04:03.118210Z"
    }
   },
   "outputs": [],
   "source": [
    "WEAT_score = sum([s(a, X, Y) for a in A]) - sum([s(b, X, Y) for b in B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac5bd78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T20:04:30.537084Z",
     "start_time": "2023-11-04T20:04:30.526409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAT score: [[[0.76391714]]\n",
      "\n",
      " [[0.69958063]]\n",
      "\n",
      " [[0.35879178]]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"WEAT score: {WEAT_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5601fc95",
   "metadata": {},
   "source": [
    "### Let's modify the code to get a single scalar value for the WEAT Score.\n",
    "\n",
    "Modifying the 's' function to ensure that it returns a scalar value instead of a matrix. Let's take the average of the cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34522ffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T20:10:56.822394Z",
     "start_time": "2023-11-04T20:10:56.782589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAT score: 0.25109671349724283\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Given word embeddings\n",
    "word_embeddings = {\n",
    "    'doctor': np.array([0.1, 0.3, 0.5]),\n",
    "    'engineer': np.array([0.2, 0.4, 0.2]),\n",
    "    'scientist': np.array([0.3, 0.1, 0.4]),\n",
    "    'nurse': np.array([0.5, 0.1, 0.3]),\n",
    "    'teacher': np.array([0.4, 0.2, 0.1]),\n",
    "    'receptionist': np.array([0.3, 0.4, 0.3]),\n",
    "    'man': np.array([0.5, 0.5, 0.5]),\n",
    "    'male': np.array([0.5, 0.4, 0.5]),\n",
    "    'boy': np.array([0.5, 0.5, 0.4]),\n",
    "    'woman': np.array([0.5, 0.2, 0.3]),\n",
    "    'female': np.array([0.5, 0.3, 0.3]),\n",
    "    'girl': np.array([0.5, 0.3, 0.4])\n",
    "}\n",
    "\n",
    "# Define our sets\n",
    "X = ['doctor', 'engineer', 'scientist']\n",
    "Y = ['nurse', 'teacher', 'receptionist']\n",
    "A = ['man', 'male', 'boy']\n",
    "B = ['woman', 'female', 'girl']\n",
    "\n",
    "def s(w, X, Y):\n",
    "    sim_X = np.mean([cosine_similarity(word_embeddings[w].reshape(1, -1), word_embeddings[x].reshape(1, -1)) for x in X])\n",
    "    sim_Y = np.mean([cosine_similarity(word_embeddings[w].reshape(1, -1), word_embeddings[y].reshape(1, -1)) for y in Y])\n",
    "    return sim_X - sim_Y\n",
    "\n",
    "WEAT_score = sum([s(a, X, Y) for a in A]) - sum([s(b, X, Y) for b in B])\n",
    "print(f\"WEAT score: {WEAT_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d2c3a",
   "metadata": {},
   "source": [
    "The WEAT score we obtained, \n",
    "0.2511, is a positive value. Here's how to interpret it in the context of the word sets:\n",
    "\n",
    "Target word sets (Occupations):\n",
    "\n",
    "-\n",
    "X: ['doctor', 'engineer', 'scientist']\n",
    "-\n",
    "Y: ['nurse', 'teacher', 'receptionist']\n",
    "Attribute word sets (Gender):\n",
    "\n",
    "-\n",
    "A: ['man', 'male', 'boy']\n",
    "-\n",
    "B: ['woman', 'female', 'girl']\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The positive WEAT score of \n",
    "0.2511\n",
    "0.2511 indicates that the words in set \n",
    "-\n",
    "A (male-associated terms) have a stronger association with the occupations in set \n",
    "-\n",
    "X (like 'doctor', 'engineer', 'scientist') than they do with occupations in set \n",
    "-\n",
    "Y (like 'nurse', 'teacher', 'receptionist'). In contrast, the words in set \n",
    "-\n",
    "B (female-associated terms) have a stronger association with occupations in set \n",
    "-\n",
    "Y.\n",
    "\n",
    "In simpler terms, based on the word embeddings you provided, there appears to be a gender bias. The male terms are more closely associated with professions like 'doctor', 'engineer', and 'scientist', while the female terms are more closely associated with 'nurse', 'teacher', and 'receptionist'.\n",
    "\n",
    "While the score is positive and indicates a bias, it's important to consider the magnitude. A score closer to 0 would suggest a weaker bias, while a score further from 0 (either positive or negative) would suggest a stronger bias. In this case, the score of 0.2511 indicates a moderate bias in the embeddings based on the chosen word sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6eed7",
   "metadata": {},
   "source": [
    "#### Demographic Diversity Analysis\n",
    "Introduction\n",
    "\n",
    "- Objective: Measure the performance of LLMs across different demographic groups.\n",
    "  * Importance of demographic parity in LLMs.\n",
    "  * Steps in Demographic Diversity Analysis\n",
    "\n",
    "- Define demographic groups.\n",
    "   * Measure model's performance for each group.\n",
    "   * Compare results to identify disparities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded567a",
   "metadata": {},
   "source": [
    "#### Let's imagine we have an LLM that's been trained to answer questions. We will assess its performance across two hypothetical demographic groups: native English speakers and non-native English speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd94925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T20:28:40.861339Z",
     "start_time": "2023-11-04T20:28:40.856046Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2d3bf45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T20:30:59.925009Z",
     "start_time": "2023-11-04T20:30:59.919037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample questions and the correct answers\n",
    "questions = {\n",
    "    \"What's the capital of France?\": \"Paris\",\n",
    "    \"Which gas do plants take in during photosynthesis?\": \"Carbon dioxide\",\n",
    "    \"Who wrote Romeo and Juliet?\": \"William Shakespeare\",\n",
    "    \"In which year did World War II end?\": \"1945\",\n",
    "    \"How many sides does a hexagon have?\": \"6\"\n",
    "}\n",
    "\n",
    "# Hypothetical responses from the LLM for native and non-native speakers\n",
    "native_responses = {\n",
    "    \"What's the capital of France?\": \"Paris\",\n",
    "    \"Which gas do plants take in during photosynthesis?\": \"Carbon dioxide\",\n",
    "    \"Who wrote Romeo and Juliet?\": \"Shakespeare\",\n",
    "    \"In which year did World War II end?\": \"1945\",\n",
    "    \"How many sides does a hexagon have?\": \"Six\"\n",
    "}\n",
    "\n",
    "non_native_responses = {\n",
    "    \"What's the capital of France?\": \"Paris\",\n",
    "    \"Which gas do plants take in during photosynthesis?\": \"Oxygen\",\n",
    "    \"Who wrote Romeo and Juliet?\": \"Shakespeare\",\n",
    "    \"In which year did World War II end?\": \"1944\",\n",
    "    \"How many sides does a hexagon have?\": \"Six\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4faaee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T20:31:12.021299Z",
     "start_time": "2023-11-04T20:31:12.013344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for native English speakers: 0.60\n",
      "Accuracy for non-native English speakers: 0.20\n"
     ]
    }
   ],
   "source": [
    "def evaluate_responses(correct_answers, responses):\n",
    "    correct_count = sum([1 for q, a in correct_answers.items() if responses[q] == a])\n",
    "    accuracy = correct_count / len(correct_answers)\n",
    "    return accuracy\n",
    "\n",
    "native_accuracy = evaluate_responses(questions, native_responses)\n",
    "non_native_accuracy = evaluate_responses(questions, non_native_responses)\n",
    "\n",
    "print(f\"Accuracy for native English speakers: {native_accuracy:.2f}\")\n",
    "print(f\"Accuracy for non-native English speakers: {non_native_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeeca68",
   "metadata": {},
   "source": [
    "Alright, let's interpret these results:\n",
    "\n",
    "1. **Accuracy for native English speakers: 0.60**\n",
    "    - This means the LLM correctly answered 60% of the questions posed by native English speakers.\n",
    "  \n",
    "2. **Accuracy for non-native English speakers: 0.20**\n",
    "    - The LLM correctly answered only 20% of the questions posed by non-native English speakers.\n",
    "\n",
    "### **Interpretation**:\n",
    "\n",
    "- There's a significant disparity in the model's performance between the two groups. The model seems to perform better for native English speakers compared to non-native speakers by a wide margin (40% difference in accuracy).\n",
    "  \n",
    "- Such a disparity might suggest that the LLM is biased in favor of native English speakers or is not adept at understanding the nuances or potential grammatical inaccuracies in questions posed by non-native speakers.\n",
    "\n",
    "### **Implications**:\n",
    "\n",
    "- If the LLM is being used in applications that cater to a global audience, this bias can be problematic. It's crucial to ensure equitable performance across diverse user groups.\n",
    "  \n",
    "- Further investigation is needed to determine the cause of this disparity. Is it because of the way questions are phrased by non-native speakers? Or is the model inherently biased due to its training data? Answering these questions can guide interventions to improve the model's performance.\n",
    "\n",
    "### **Recommendations**:\n",
    "\n",
    "1. **Data Augmentation**: Introduce more diverse training data, especially data representing non-native English speakers.\n",
    "2. **Feedback Loop**: Allow users to provide feedback on incorrect answers, and use this feedback to continuously train and improve the model.\n",
    "3. **Bias Mitigation Techniques**: Apply techniques designed to reduce bias in AI models.\n",
    "4. **Clear Communication**: If deploying the model in its current state, communicate its limitations to users.\n",
    "\n",
    "In summary, the results indicate a need for further refinement and calibration of the LLM to ensure it serves all user groups equitably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c74196",
   "metadata": {},
   "source": [
    "#### Benefits of Bias Analysis\n",
    "- Ensuring fairness and inclusivity in AI systems.\n",
    "- Enhancing trust and acceptance among users.\n",
    "- Aligning with ethical considerations and societal norms.\n",
    "#### Challenges and Considerations\n",
    "- The subjectivity of defining biases.\n",
    "- The trade-offs between accuracy and fairness.\n",
    "- The importance of continuous monitoring and updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f21ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
